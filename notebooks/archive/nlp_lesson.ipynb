{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a26c1-c306-4ed0-864f-e29bb064efdd",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing, or NLP, is a field at the intersection of computer science, artificial intelligence, and linguistics. It involves the development of algorithms and systems that enable computers to understand, interpret, and generate human language. NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way.\n",
    "\n",
    "NLP helps resolve ambiguity in language and adds useful numeric structure to the data for many downstream applications, such as speech recognition or text analytics.\n",
    "\n",
    "![nlp](../assets/nlp.png)\n",
    "\n",
    "## Challenges in NLP\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "\n",
    "![pos](../assets/pos.jpeg)\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text, such as noun, verb, adjective, etc. This is challenging because:\n",
    "\n",
    "- **Ambiguity**: A word can have multiple parts of speech based on the context. For example, \"book\" can be a noun (\"I read a book\") or a verb (\"Book a table\").\n",
    "- **Contextual Use**: Words may be used in a figurative sense, which can confuse POS taggers.\n",
    "- **New Words**: New words, slang, and jargon keep emerging, and POS taggers need regular updates to handle them.\n",
    "\n",
    "### Text Segmentation\n",
    "\n",
    "Text segmentation involves dividing text into meaningful units, such as sentences or topics. Challenges include:\n",
    "\n",
    "![senseg](../assets/sentence_segmentation.jpeg)\n",
    "\n",
    "- **Sentence Boundary Detection**: Punctuation marks like periods can be used for abbreviations, decimals, etc., and not always to end sentences.\n",
    "- **Tokenization**: Different languages and scripts have different tokenization rules, and some don't use whitespace.\n",
    "- **Topic Segmentation**: Identifying topic shifts in a text requires understanding of the content, which is a non-trivial task.\n",
    "\n",
    "### Word Sense Disambiguation\n",
    "\n",
    "Word sense disambiguation is the task of determining which sense of a word is active in a given context. Challenges include:\n",
    "\n",
    "- **Polysemy**: Many words have multiple meanings, and identifying the correct one is difficult without deep understanding.\n",
    "- **Limited Context**: Sometimes the surrounding text is not enough to determine the word sense.\n",
    "- **Lack of Resources**: For less-resourced languages, there might not be enough data to train disambiguation systems.\n",
    "\n",
    "> - Many plants and animals live in the rainforest.\n",
    "> - The manufacturing plant produced widgets.\n",
    "\n",
    "### Syntax Disambiguation\n",
    "\n",
    "Syntax disambiguation deals with the different ways in which words can be combined to form sentences. Challenges here include:\n",
    "\n",
    "- **Structural Ambiguity**: Sentences can often be parsed in multiple ways (\"I saw the man with the telescope\").\n",
    "- **Complex Constructions**: Some languages have free word order or allow for nested clauses, making parsing difficult.\n",
    "- **Idiomatic Expressions**: Phrases that don't follow standard syntax rules can confuse parsers.\n",
    "\n",
    "> - Annie hit a man with an umbrella.\n",
    "> - I shot an elephant in my pyjamas.\n",
    "> - The tourist saw the woman with a telescope.\n",
    "\n",
    "### Imperfect or Irregular Input\n",
    "\n",
    "Language is often messy and unpredictable, leading to challenges such as:\n",
    "\n",
    "- **Typos and Spelling Errors**: Mistakes in writing can lead to misinterpretation by NLP systems.\n",
    "- **Non-standard Language**: Use of slang, abbreviations, and non-standard grammar can be problematic.\n",
    "- **Multilingual Text**: Text containing multiple languages can complicate processing.\n",
    "\n",
    "## Applications of NLP\n",
    "\n",
    "- **Text Classification**: Assigning categories or labels to text, such as spam detection in email services.\n",
    "- **Machine Translation**: Translating text from one language to another, like Google Translate.\n",
    "- **Sentiment Analysis**: Identifying the sentiment of text, used in social media monitoring and market research.\n",
    "- **Chatbots and Virtual Assistants**: Powering conversational agents like Siri, Alexa, and customer service bots.\n",
    "- **Information Extraction**: Extracting structured information from unstructured text, such as named entity recognition.\n",
    "- **Summarization**: Generating a shortened version of a text, retaining its most important information.\n",
    "- **Speech Recognition**: Translating spoken language into text, used in voice user interfaces.\n",
    "- **Question Answering**: Building systems that automatically answer questions posed by humans in a natural language (ChatGPT).\n",
    "\n",
    "## Brief History of NLP\n",
    "\n",
    "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a fundamental goal of natural language processing.\n",
    "\n",
    "### Milestones in the History of NLP\n",
    "\n",
    "- **1950s**: The era of symbolic NLP, rule-based systems that tried to encode human knowledge and grammar rules into computers.\n",
    "- **1960s**: Development of the first chatbot, ELIZA, and further work on machine translation.\n",
    "- **1970s-1980s**: The rise of computational linguistics and the development of more sophisticated models for handling syntax and semantics.\n",
    "- **1990s**: Introduction of statistical NLP, leveraging large amounts of data and statistical methods to process language.\n",
    "- **2000s**: The emergence of machine learning in NLP, with systems beginning to learn from data rather than relying on hand-coded rules.\n",
    "- **2010s-Present**: The rise of deep learning has revolutionized NLP, leading to the development of models like BERT and GPT that can handle complex language tasks with unprecedented accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1313cd2-e658-4864-8a60-2f0dc263b96a",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is a critical step in NLP. It involves preparing and cleaning text data for further analysis and modeling. The goal is to simplify the text and remove any noise that might distract the machine learning algorithms from understanding the core content.\n",
    "\n",
    "Raw text data is often messy and unstructured, with various issues:\n",
    "\n",
    "- Irrelevant characters and symbols\n",
    "- Inconsistent formatting\n",
    "- Typos and spelling errors\n",
    "- Diverse languages and slang\n",
    "- Stopwords (commonly used words that may not be useful in analysis)\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, called *tokens*. Tokens can be words, numbers, or punctuation marks. It's the first step in turning unstructured text into a form that can be analyzed.\n",
    "\n",
    "### White-space Tokenization\n",
    "\n",
    "This is the simplest form of tokenization. It splits the text by white spaces, including spaces, tabs, and new line characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b71bf2-b565-4882-b092-5cca88a7b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun.']\n"
     ]
    }
   ],
   "source": [
    "def whitespace_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Example usage:\n",
    "text = \"Natural language processing is fun.\"\n",
    "tokens = whitespace_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2701c3-c486-4eca-af0a-3de088d7f194",
   "metadata": {},
   "source": [
    "### Punctuation-based Tokenization\n",
    "\n",
    "This method not only splits by white spaces but also considers punctuation marks as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d89ec8-9f7c-4c8e-a0a1-933e194f5908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def punctuation_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"Natural language processing is fun!\"\n",
    "tokens = punctuation_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dded96-252a-416d-8ea5-eee987770258",
   "metadata": {},
   "source": [
    "### Using NLP Libraries for Tokenization\n",
    "\n",
    "Libraries like `NLTK` and `spaCy` provide robust tokenization functions that handle edge cases and are more sophisticated than the simple white-space or punctuation-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899927f5-01e9-4a61-bec1-e9664930e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zanelim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5243c0d8-b212-4ab0-ab7f-5df1b1881403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing is fun!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8dbed7-a987-49b2-97b3-3aec6b6f6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Download the spaCy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438537c7-65b9-4ba0-a9bb-70e81810f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57aa0f5-8945-4c4f-baac-d33aa11450a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing is fun!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21470992-244d-4579-8225-4cb6cf4afb9c",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "Text normalization involves transforming text into a more uniform format to improve the performance of text analysis algorithms. Two common text normalization techniques are *stemming* and *lemmatization*.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is a process of reducing words to their word stem, base, or root form—generally a written word form. The idea is to remove affixes (prefixes and suffixes) from words to get to the core meaning of the word.\n",
    "\n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This process is quite crude and a stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n",
    "\n",
    "### Porter Stemmer\n",
    "\n",
    "The Porter Stemming Algorithm is one of the oldest and most commonly used algorithms. It's designed for the English language and has a series of rules to determine the stripping of suffixes.\n",
    "\n",
    "### Snowball Stemmer\n",
    "\n",
    "The Snowball Stemmer, also known as the English Stemmer or Porter2 Stemmer, is a slightly improved version of the Porter stemmer and is part of a larger framework called Snowball. It offers stemmers for several languages besides English.\n",
    "\n",
    "### Advantages and Disadvantages of Stemming\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement and fast to run.\n",
    "- Reduces the corpus of words the model is exposed to.\n",
    "- Often improves the performance of text classification models.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Can produce stems that are not actual words.\n",
    "- Sometimes too aggressive, cutting off too much of the word and changing the meaning.\n",
    "- Does not consider the context of the word, which can lead to inaccuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926b0031-0c3d-4008-aafa-8638b86028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399ccb3c-13f2-4f56-8d90-cacdfd7359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e20d0860-992a-456b-87a4-a2af568c9add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['run', 'runner', 'run', 'ran', 'run', 'easili', 'fairli', 'better', 'mice', 'feet']\n",
      "Snowball Stemmer: ['run', 'runner', 'run', 'ran', 'run', 'easili', 'fair', 'better', 'mice', 'feet']\n"
     ]
    }
   ],
   "source": [
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly', 'better', 'mice', 'feet']\n",
    "\n",
    "porter_stems = [porter.stem(word) for word in words]\n",
    "print(f\"Porter Stemmer: {porter_stems}\")\n",
    "\n",
    "snowball_stems = [snowball.stem(word) for word in words]\n",
    "print(f\"Snowball Stemmer: {snowball_stems}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3a6cd-45fe-4e87-9b58-9f2e3f28982b",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "While stemming often involves rule-based chopping of ends of words, lemmatization involves a linguistic approach to reduce a word to its base or root form. Lemmatization uses vocabulary and morphological analysis, often with the aid of part-of-speech tagging, to return the base or dictionary form of a word, known as the lemma.\n",
    "\n",
    "### The Role of Part-of-Speech Tagging in Lemmatization\n",
    "\n",
    "Part-of-speech (POS) tagging is crucial in lemmatization because many words have different lemmas based on their part of speech in a sentence. For example, the word \"saw\" can be a verb or a noun, and the lemma would differ accordingly (\"see\" for the verb, \"saw\" for the noun).\n",
    "\n",
    "`nltk` or `spacy` contain pre-trained models for POS tagging and lemmatization.\n",
    "\n",
    "### Advantages and Disadvantages of Lemmatization\n",
    "\n",
    "**Advantages:**\n",
    "- Produces lemmas, which are actual words, improving interpretability.\n",
    "- More accurate than stemming as it considers the context.\n",
    "\n",
    "**Disadvantages:**\n",
    "- More computationally expensive than stemming.\n",
    "- Requires additional information (POS tags).\n",
    "- May not improve performance significantly more than stemming for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ee9e22-0907-4a5f-ba77-1ad2db3a87f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zanelim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zanelim/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "351ee21f-7a5e-43fa-9f6c-7ce3d16c9883",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7136553-ced5-4280-bf20-7a5319c4b1f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to convert POS tag to a format recognized by the lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09b374d0-0b19-4ae0-b426-beec511f22dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'runner', 'run', 'ran', 'run', 'easily', 'fairly', 'well', 'mouse', 'foot']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize words with POS tags\n",
    "lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b8b01d-d58d-4491-abf2-18bce5821101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use spacy on another example to demonstrate pos and lemmatization\n",
    "sentence = \"The striped bats are hanging on their feet for better sleep.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2df23730-5b01-48e7-b343-18717682b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30d8159f-65cf-46e0-8abf-61da4d733d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     Lemma    POS    Tag    Explanation\n",
      "\n",
      "The      the      DET    DT     determiner\n",
      "striped  stripe   VERB   VBN    verb, past participle\n",
      "bats     bat      NOUN   NNS    noun, plural\n",
      "are      be       AUX    VBP    verb, non-3rd person singular present\n",
      "hanging  hang     VERB   VBG    verb, gerund or present participle\n",
      "on       on       ADP    IN     conjunction, subordinating or preposition\n",
      "their    their    PRON   PRP$   pronoun, possessive\n",
      "feet     foot     NOUN   NNS    noun, plural\n",
      "for      for      ADP    IN     conjunction, subordinating or preposition\n",
      "better   well     ADJ    JJR    adjective, comparative\n",
      "sleep    sleep    NOUN   NN     noun, singular or mass\n",
      ".        .        PUNCT  .      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# POS tagging and lemmatization\n",
    "print(f\"{'Text':{8}} {'Lemma':{8}} {'POS':{6}} {'Tag':{6}} {'Explanation'}\")\n",
    "print()\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{8}} {token.lemma_:{8}} {token.pos_:{6}} {token.tag_:{6}} {spacy.explain(token.tag_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce78d490-86b4-4faf-a120-fc757e1840bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stripe bat be hang on their foot for well sleep .\n"
     ]
    }
   ],
   "source": [
    "# Output the lemmatized form of each word\n",
    "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d3ca9-a395-46ab-a0dc-6ccf8115629c",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "Language modeling is a critical task that deals with predicting the probability of a sequence of words. It is used in various applications such as speech recognition, machine translation, and text generation.\n",
    "\n",
    "## Formula\n",
    "\n",
    "A language model is a probabilistic model that assigns a probability to a sequence of words, effectively capturing the likelihood that the sequence will occur in a language. In mathematical terms, given a sequence of words $ w_1, w_2, \\ldots, w_n $, the language model estimates the probability:\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) $$\n",
    "\n",
    "This probability can be decomposed using the chain rule of probability as:\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot \\ldots \\cdot P(w_n | w_1, w_2, \\ldots, w_{n-1}) $$\n",
    "\n",
    "## N-gram Models\n",
    "\n",
    "An n-gram is a contiguous sequence of $ n $ items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. In the context of language modeling, we are typically talking about words. It approximates the probability of a word sequence by only considering the $ n-1 $ previous words. This is known as the Markov assumption.\n",
    "\n",
    "### Unigram Models\n",
    "\n",
    "A unigram model is the simplest form of a statistical language model. It assumes that the probability of a word is independent of the words before it.\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) = P(w_1) \\cdot P(w_2) \\cdot \\ldots \\cdot P(w_n) $$\n",
    "\n",
    "### Bigram Models\n",
    "\n",
    "A bigram model, also known as a 2-gram model, assumes that the probability of a word depends only on the immediately preceding word.\n",
    "\n",
    "$$ P(w_n | w_1, w_2, \\ldots, w_{n-1}) \\approx P(w_n | w_{n-1}) = \\frac{Count(w_{n-1}, w_n)}{Count(w_{n-1})} $$\n",
    "\n",
    "### Trigram Models and Higher-Order Models\n",
    "\n",
    "Trigram models extend this to consider the two preceding words, and higher-order models consider more history. However, as the history increases, these models become more complex and require more data to estimate the probabilities accurately.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "**Sparsity**: As $ n $ increases, the likelihood of encountering unseen n-grams (those not present in the training corpus) increases, leading to sparsity.\n",
    "\n",
    "**Curse of Dimensionality**: The number of possible n-grams increases exponentially with $ n $, which leads to a combinatorial explosion in the number of parameters to be estimated.\n",
    "\n",
    "### Smoothing Techniques\n",
    "\n",
    "Smoothing techniques are used to handle the issue of zero probabilities for unseen n-grams. Common techniques include:\n",
    "\n",
    "- **Add-One (Laplace) Smoothing**: Adding one to all the n-gram counts.\n",
    "- **Add-k Smoothing**: Adding a small constant $ k $ to the counts.\n",
    "- **Backoff and Interpolation**: Using lower-order n-gram probabilities when higher-order n-grams have zero counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad8b59a7-418d-480b-9e3b-3c14f6e0eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71e84026-51e3-438b-9426-7e3cc8ca44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4aa8cae6-59b4-40a6-b9a2-7d0113d4f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokens = nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a24f20d-06f0-43e3-b307-7db3e98c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bigram frequencies\n",
    "bigram_freqs = Counter(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e02f307c-accf-47f5-80e5-b77728b1cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of bigrams\n",
    "total_bigrams = sum(bigram_freqs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "893b4af3-5f8e-4766-b4d6-e156c5635b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bigram probabilities\n",
    "bigram_probs = {bigram: freq / total_bigrams for bigram, freq in bigram_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d39622f-e3ad-45ac-94cd-fd82d70e3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ('I', 'am'): 0.125\n",
      "Probability of ('am', 'Sam'): 0.0625\n",
      "Probability of ('Sam', '.'): 0.0625\n",
      "Probability of ('.', 'Sam'): 0.0625\n",
      "Probability of ('Sam', 'I'): 0.0625\n",
      "Probability of ('am', '.'): 0.0625\n",
      "Probability of ('.', 'I'): 0.0625\n",
      "Probability of ('I', 'do'): 0.0625\n",
      "Probability of ('do', 'not'): 0.0625\n",
      "Probability of ('not', 'like'): 0.0625\n",
      "Probability of ('like', 'green'): 0.0625\n",
      "Probability of ('green', 'eggs'): 0.0625\n",
      "Probability of ('eggs', 'and'): 0.0625\n",
      "Probability of ('and', 'ham'): 0.0625\n",
      "Probability of ('ham', '.'): 0.0625\n"
     ]
    }
   ],
   "source": [
    "# Display bigram probabilities\n",
    "for bigram, prob in bigram_probs.items():\n",
    "    print(f\"Probability of {bigram}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be94fb-c2f3-4219-a92a-85aa65cc5c0c",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "\n",
    "The Vector Space Model (VSM) is a mathematical model used to represent text documents as vectors of identifiers, such as index terms. It is used in information retrieval and text mining to measure the similarity between documents. In VSM, each dimension corresponds to a separate term, and the value in each dimension represents the significance of the term in the document.\n",
    "\n",
    "## Term-Document Matrix\n",
    "\n",
    "In VSM, a Term-Document Matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the collection of documents. In a Term-Document Matrix, rows correspond to terms in the corpus while columns correspond to documents. Each entry in this matrix denotes the frequency or the weight of a term in a document.\n",
    "\n",
    "Here's a simple example of a Term-Document Matrix for five documents:\n",
    "\n",
    "![tf](../assets/tf.png)\n",
    "\n",
    "Such matrices are often sparse since not all words appear in all documents.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "The TF-IDF value is calculated as follows:\n",
    "\n",
    "- **Term Frequency (TF)**, which measures how frequently a term occurs in a document. It is calculated as the number of times a term `t` appears in a document `d`, divided by the total number of terms in the document.\n",
    "\n",
    "$$ TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**, which measures how important a term is within the entire corpus. It is calculated as the logarithm of the number of documents in the corpus divided by the number of documents where the term `t` appears.\n",
    "\n",
    "$$ IDF(t, D) = \\log \\left( \\frac{\\text{Total number of documents in corpus } D}{\\text{Number of documents with term } t} \\right) $$\n",
    "\n",
    "- **TF-IDF**, the product of TF and IDF:\n",
    "\n",
    "$$ TF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D) $$\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is a measure used to determine how similar two documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. This metric is, therefore, a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The cosine similarity of two documents will range from 0 to 1, where 0 means no similarity and 1 means the same content.\n",
    "\n",
    "The formula for calculating the cosine similarity between two vectors $ A $ and $ B $ is:\n",
    "\n",
    "$$ \\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i \\times B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}} $$\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Assumes term independence, which is not always the case.\n",
    "- Does not capture the semantic relationship between words.\n",
    "- High-dimensional and sparse vectors due to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52c8953a-2faa-4755-b6dc-37f80fddd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d6856ef-bedf-4c51-8f9b-db4f6a0d9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    'The sky is blue',\n",
    "    'The sun is bright',\n",
    "    'The sun in the sky is bright',\n",
    "    'We can see the shining sun, the bright sun'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07d0c98d-f291-4d27-a151-eebfcf8b05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1b5897a-7f40-4fb4-aae1-e01ce9397d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b260969c-be3e-46b1-bb43-c897fa88bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65919112 0.         0.         0.         0.42075315 0.\n",
      "  0.         0.51971385 0.         0.34399327 0.        ]\n",
      " [0.         0.52210862 0.         0.         0.52210862 0.\n",
      "  0.         0.         0.52210862 0.42685801 0.        ]\n",
      " [0.         0.3218464  0.         0.50423458 0.3218464  0.\n",
      "  0.         0.39754433 0.3218464  0.52626104 0.        ]\n",
      " [0.         0.23910199 0.37459947 0.         0.         0.37459947\n",
      "  0.37459947 0.         0.47820398 0.39096309 0.37459947]]\n"
     ]
    }
   ],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5866e768-a9be-4448-898e-26cf8487a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36651513 1.         0.72875508 0.54139736]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cosine Similarity between the 2nd document with all others\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1], tfidf_matrix)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb92b21c-ecf0-469e-a761-1242ca42d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize another TF-IDF Vectorizer, with stop words removal\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0c27285-3f68-443c-9062-316d5498a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31960d6a-7e0b-4790-a657-10dd24f9c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78528828 0.         0.         0.6191303  0.        ]\n",
      " [0.         0.70710678 0.         0.         0.70710678]\n",
      " [0.         0.53256952 0.         0.65782931 0.53256952]\n",
      " [0.         0.36626037 0.57381765 0.         0.73252075]]\n"
     ]
    }
   ],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5e6f0c0-5c97-4a64-a69a-8a72573a0efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         0.75316704 0.77695558]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cosine Similarity between the 2nd document with all others\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1], tfidf_matrix)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d386884-fe61-41d7-9bb5-9ee3cd0abaf8",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## What are Word Embeddings?\n",
    "\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging NLP problems.\n",
    "\n",
    "In essence, word embeddings are a form of word representation that bridges the human understanding of language to that of a machine. They are mappings to a high-dimensional space, where words that have similar meanings are located in close proximity to one another.\n",
    "\n",
    "## Why Use Word Embeddings?\n",
    "\n",
    "Traditional language models often represent words as one-hot encoded vectors where each word is represented by a vector with a dimensionality equal to the size of the vocabulary. The main issue with this approach is that the resulting vectors are sparse and do not capture any information about word relationships.\n",
    "\n",
    "Word embeddings address this by providing a dense representation where similar words have a similar encoding. Importantly, word embeddings can capture nuances about words, such as their semantic and syntactic information.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2Vec is a popular algorithm to produce word embeddings by training a neural network with a single hidden layer. Word2Vec comes with two model architectures:\n",
    "\n",
    "### Architecture (CBOW and Skip-gram)\n",
    "\n",
    "- **Continuous Bag of Words (CBOW)**: The CBOW model predicts the current word based on the context, and the context is represented as a bag of words. Hence, the order of words in the context does not influence prediction (bag of words model).\n",
    "\n",
    "- **Skip-gram**: The Skip-gram model works in the reverse manner, it tries to predict the context for a given word.\n",
    "\n",
    "### Training Word2Vec\n",
    "\n",
    "The Word2Vec model is trained with either one of these architectures, each of which has the objective to learn word vector representations that are good at predicting their context in the input corpus.\n",
    "\n",
    "For a given word $ w_I $ and its context $ w_O $ in the corpus, the objective of the Skip-gram model is to maximize the following log probability:\n",
    "\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t) $$\n",
    "\n",
    "where $ c $ is the size of the training context (which can be a function of the center word $ w_t $). The $ p(w_{t+j} | w_t) $ is defined using the softmax function:\n",
    "\n",
    "$$ p(w_O | w_I) = \\frac{\\exp({v'_{w_O}}^T v_{w_I})}{\\sum_{w=1}^{W} \\exp({v'_w}^T v_{w_I})} $$\n",
    "\n",
    "where $ v_w $ and $ v'_w $ are the \"input\" and \"output\" vector representations of $ w $, and $ W $ is the number of words in the vocabulary.\n",
    "\n",
    "### Applications\n",
    "\n",
    "One of the fascinating properties of Word2Vec embeddings is their ability to capture analogies and relationships between words. The classic example often cited to demonstrate this is the relationship between \"man\" and \"woman,\" and \"king\" and \"queen.\"\n",
    "\n",
    "Word2Vec can capture these relationships because it learns vector representations of words in such a way that the geometric relationships between the vectors capture semantic relationships between the words. For instance, the difference between the vectors for \"man\" and \"woman\" often encodes the concept of gender. Similarly, the difference between \"king\" and \"queen\" captures the same concept of gender.\n",
    "\n",
    "![word2vec](../assets/word2vec.png)\n",
    "\n",
    "In practice, this means that if we take the vector for \"king,\" subtract the vector for \"man,\" and then add the vector for \"woman,\" we end up with a vector that is close to the vector for \"queen.\" Mathematically, this relationship can be represented as:\n",
    "\n",
    "$$ \\textbf{vector}('king') - \\textbf{vector}('man') + \\textbf{vector}('woman') \\approx \\textbf{vector}('queen') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a58613-6e36-452b-9424-9971226313a2",
   "metadata": {},
   "source": [
    "Let's use the `20 Newsgroups` dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0c8cbdf-240c-46fe-9604-6ef9a7435b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "edadcf3b-2519-4a2c-a9bb-46fd3fa662a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the 20 newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43098938-fe4f-4164-8379-53e24e6a4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text using gensim's simple_preprocess\n",
    "# This will tokenize the text, lowercasing, and remove punctuation\n",
    "corpus = [simple_preprocess(doc) for doc in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "187622c9-dec4-4276-bf9d-e6ffccbc4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8d24198-9613-489f-ab53-8674bc4719a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'computer':\n",
      "electronics: 0.7345\n",
      "tech: 0.7098\n",
      "project: 0.7091\n",
      "network: 0.7038\n",
      "software: 0.6971\n",
      "computers: 0.6965\n",
      "workstation: 0.6948\n",
      "programming: 0.6927\n",
      "macintosh: 0.6907\n",
      "lab: 0.6882\n"
     ]
    }
   ],
   "source": [
    "# Explore the model\n",
    "# Let's find the most similar words to 'computer'\n",
    "similar_words = model.wv.most_similar('computer', topn=10)\n",
    "print(\"Most similar words to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa836c66-60af-48ef-b586-a40f517954ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'car':\n",
      "bike: 0.8113\n",
      "tires: 0.7053\n",
      "seat: 0.6932\n",
      "dealer: 0.6903\n",
      "bought: 0.6852\n",
      "engine: 0.6800\n",
      "helmet: 0.6658\n",
      "driving: 0.6630\n",
      "battery: 0.6599\n",
      "riding: 0.6598\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar('car', topn=10)\n",
    "print(\"Most similar words to 'car':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da3ca4-9beb-4bad-adb1-bb42771a9971",
   "metadata": {},
   "source": [
    "# Advanced Embeddings\n",
    "\n",
    "## GloVe\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations of words. It stands for \"Global Vectors for Word Representation,\" and it is specifically designed to capture global word-word co-occurrence statistics from a corpus. The resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "### How GloVe Works\n",
    "\n",
    "The GloVe model is trained on the non-zero elements in a word-word co-occurrence matrix, which tabulates how frequently words co-occur with one another in a given corpus. Instead of using window-based co-occurrence, GloVe constructs an explicit word-context or word-word co-occurrence matrix using statistics across the whole text corpus.\n",
    "\n",
    "The model then uses matrix factorization techniques to yield a word vector space, where the difference between any two word vectors aims to approximate the logarithm of the words' probability of co-occurrence.\n",
    "\n",
    "Given a co-occurrence matrix $X$, where $X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$, the GloVe model aims to learn a vector $w_i$ for each word $i$ such that the dot product $w_i^T w_j$ is proportional to the logarithm of $X_{ij}$.\n",
    "\n",
    "The training objective of GloVe is:\n",
    "\n",
    "$$ J = \\sum_{i,j=1}^V f(X_{ij}) (w_i^T w_j + b_i + b_j - \\log X_{ij})^2 $$\n",
    "\n",
    "where $V$ is the size of the vocabulary, $b_i$ and $b_j$ are scalar bias terms for words $i$ and $j$, and $f$ is a weighting function that helps prevent learning from large co-occurrence counts.\n",
    "\n",
    "## FastText\n",
    "\n",
    "FastText is another word embedding method that extends Word2Vec to consider subword information. This means that it takes into account the internal structure of words while learning word representations. FastText is particularly useful for languages with rich morphology and for understanding words outside the training vocabulary.\n",
    "\n",
    "### How FastText Works\n",
    "\n",
    "FastText represents each word as a bag of character n-grams, in addition to the word itself. This means that the word \"apple\" with $n=3$ would be represented as the following n-grams: \"<ap\", \"app\", \"ppl\", \"ple\", \"le>\" (where \"<\" and \">\" are added to denote the beginning and end of the word, respectively).\n",
    "\n",
    "The model then learns vector representations for these character n-grams, and the word vector is computed as the sum of the n-gram vectors. This allows FastText to produce representations for words not seen during training by summing the vectors of its component n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09ce6c70-18fc-4371-a8eb-3caff7a77a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3b9b318-fbc0-4a6f-9088-8efb6bbffb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12580731, 16476320)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the FastText model\n",
    "# The `vector_size` parameter specifies the dimensionality of the word vectors,\n",
    "# `window` specifies the maximum distance between the current and predicted word within a sentence\n",
    "# `min_count` specifies the minimum count of words to consider\n",
    "# `workers` specifies the number of worker threads to train the model.\n",
    "ft_model = FastText(vector_size=100, window=5, min_count=5, workers=4)\n",
    "ft_model.build_vocab(corpus_iterable=corpus)\n",
    "ft_model.train(corpus_iterable=corpus, total_examples=len(corpus), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8ba15ef9-aaeb-44b7-a5fc-4b03806de2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'computer':\n",
      "microcomputer: 0.9598\n",
      "supercomputer: 0.9588\n",
      "compute: 0.9262\n",
      "computrac: 0.9004\n",
      "compusa: 0.8864\n",
      "computers: 0.8767\n",
      "compuadd: 0.8644\n",
      "supercomputers: 0.8628\n",
      "compulink: 0.8622\n",
      "computes: 0.8613\n"
     ]
    }
   ],
   "source": [
    "# Let's find the most similar words to 'computer'\n",
    "similar_words = ft_model.wv.most_similar('computer', topn=10)\n",
    "print(\"Most similar words to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911d3ca-f7b6-4291-9902-2b1a7dd9d6ce",
   "metadata": {},
   "source": [
    "# Text Classification with Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong independence assumptions between the features. They are among the most straightforward and effective algorithms used in machine learning and natural language processing (NLP) for text classification tasks, such as spam filtering and sentiment analysis.\n",
    "\n",
    "## Understanding Naive Bayes\n",
    "\n",
    "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For a class variable \\(y\\) and a dependent feature vector \\(x_1\\) through \\(x_n\\), Bayes' theorem states the following relationship:\n",
    "\n",
    "$$ P(y \\mid x_1, \\ldots, x_n) = \\frac{P(y) P(x_1, \\ldots, x_n \\mid y)}{P(x_1, \\ldots, x_n)} $$\n",
    "\n",
    "In the Naive Bayes classification, we are interested in finding the class with the highest probability, given the features. The \"naive\" assumption of conditional independence between every pair of features given the value of the class variable simplifies the computation, as follows:\n",
    "\n",
    "$$ P(y \\mid x_1, \\ldots, x_n) \\propto P(y) \\prod_{i=1}^n P(x_i \\mid y) $$\n",
    "\n",
    "Since we are only interested in the class with the maximum probability, we can ignore the denominator and use the following classification rule:\n",
    "\n",
    "$$ \\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^n P(x_i \\mid y) $$\n",
    "\n",
    "## Naive Bayes in NLP\n",
    "\n",
    "In NLP, Naive Bayes classifiers are commonly applied to text classification problems. When dealing with text, the features are usually the frequency or presence of words. For example, in a spam filtering application, the features might be the presence or frequency of specific words or sequences of words in an email.\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "\n",
    "The Multinomial Naive Bayes classifier is a specific instance of a Naive Bayes classifier which is widely used for document classification problems. It accounts for the number of occurrences of each word (term frequency) for classification.\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "\n",
    "The Bernoulli Naive Bayes classifier is suitable when your feature vectors are binary (i.e., 0s and 1s). An example might be text classification with a 'bag of words' model where the 1s & 0s represent the presence or absence of a word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4094f210-d4c4-4e06-bc49-c932402f2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0923eee0-df30-4ba5-b955-c4819240bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the dataset\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b0a2518-2fd1-466b-bab9-682d11979c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that vectorizes the data then applies Multinomial Naive Bayes classifier\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cf77f68-4a39-4e9e-b077-1c17a7c9d666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(newsgroups_train.data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92e9f771-6d26-428d-8c0a-e7d4618c6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the categories of the test data\n",
    "predicted_categories = model.predict(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6920ff83-edde-4d2c-b17c-4f3ba91bc8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.92      0.90      0.91       319\n",
      "         comp.graphics       0.95      0.95      0.95       389\n",
      "               sci.med       0.96      0.91      0.93       396\n",
      "soc.religion.christian       0.91      0.97      0.94       398\n",
      "\n",
      "              accuracy                           0.93      1502\n",
      "             macro avg       0.93      0.93      0.93      1502\n",
      "          weighted avg       0.93      0.93      0.93      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(classification_report(newsgroups_test.target, predicted_categories, target_names=newsgroups_test.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
