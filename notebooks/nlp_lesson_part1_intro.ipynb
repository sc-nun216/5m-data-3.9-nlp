{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a26c1-c306-4ed0-864f-e29bb064efdd",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing, or NLP, is a field at the intersection of computer science, artificial intelligence, and linguistics. It involves the development of algorithms and systems that enable computers to understand, interpret, and generate human language. NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way.\n",
    "\n",
    "NLP helps resolve ambiguity in language and adds useful numeric structure to the data for many downstream applications, such as speech recognition or text analytics.\n",
    "\n",
    "![nlp](../assets/nlp.png)\n",
    "\n",
    "## Challenges in NLP\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "\n",
    "![pos](../assets/pos.jpeg)\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text, such as noun, verb, adjective, etc. This is challenging because:\n",
    "\n",
    "- **Ambiguity**: A word can have multiple parts of speech based on the context. For example, \"book\" can be a noun (\"I read a book\") or a verb (\"Book a table\").\n",
    "- **Contextual Use**: Words may be used in a figurative sense, which can confuse POS taggers.\n",
    "- **New Words**: New words, slang, and jargon keep emerging, and POS taggers need regular updates to handle them.\n",
    "\n",
    "### Text Segmentation\n",
    "\n",
    "Text segmentation involves dividing text into meaningful units, such as sentences or topics. Challenges include:\n",
    "\n",
    "![senseg](../assets/sentence_segmentation.jpeg)\n",
    "\n",
    "- **Sentence Boundary Detection**: Punctuation marks like periods can be used for abbreviations, decimals, etc., and not always to end sentences.\n",
    "- **Tokenization**: Different languages and scripts have different tokenization rules, and some don't use whitespace.\n",
    "- **Topic Segmentation**: Identifying topic shifts in a text requires understanding of the content, which is a non-trivial task.\n",
    "\n",
    "### Word Sense Disambiguation\n",
    "\n",
    "Word sense disambiguation is the task of determining which sense of a word is active in a given context. Challenges include:\n",
    "\n",
    "- **Polysemy**: Many words have multiple meanings, and identifying the correct one is difficult without deep understanding.\n",
    "- **Limited Context**: Sometimes the surrounding text is not enough to determine the word sense.\n",
    "- **Lack of Resources**: For less-resourced languages, there might not be enough data to train disambiguation systems.\n",
    "\n",
    "> - Many plants and animals live in the rainforest.\n",
    "> - The manufacturing plant produced widgets.\n",
    "\n",
    "### Syntax Disambiguation\n",
    "\n",
    "Syntax disambiguation deals with the different ways in which words can be combined to form sentences. Challenges here include:\n",
    "\n",
    "- **Structural Ambiguity**: Sentences can often be parsed in multiple ways (\"I saw the man with the telescope\").\n",
    "- **Complex Constructions**: Some languages have free word order or allow for nested clauses, making parsing difficult.\n",
    "- **Idiomatic Expressions**: Phrases that don't follow standard syntax rules can confuse parsers.\n",
    "\n",
    "> - Annie hit a man with an umbrella.\n",
    "> - I shot an elephant in my pyjamas.\n",
    "> - The tourist saw the woman with a telescope.\n",
    "\n",
    "### Imperfect or Irregular Input\n",
    "\n",
    "Language is often messy and unpredictable, leading to challenges such as:\n",
    "\n",
    "- **Typos and Spelling Errors**: Mistakes in writing can lead to misinterpretation by NLP systems.\n",
    "- **Non-standard Language**: Use of slang, abbreviations, and non-standard grammar can be problematic.\n",
    "- **Multilingual Text**: Text containing multiple languages can complicate processing.\n",
    "\n",
    "## Applications of NLP\n",
    "\n",
    "- **Text Classification**: Assigning categories or labels to text, such as spam detection in email services.\n",
    "- **Machine Translation**: Translating text from one language to another, like Google Translate.\n",
    "- **Sentiment Analysis**: Identifying the sentiment of text, used in social media monitoring and market research.\n",
    "- **Chatbots and Virtual Assistants**: Powering conversational agents like Siri, Alexa, and customer service bots.\n",
    "- **Information Extraction**: Extracting structured information from unstructured text, such as named entity recognition.\n",
    "- **Summarization**: Generating a shortened version of a text, retaining its most important information.\n",
    "- **Speech Recognition**: Translating spoken language into text, used in voice user interfaces.\n",
    "- **Question Answering**: Building systems that automatically answer questions posed by humans in a natural language (ChatGPT).\n",
    "\n",
    "## Brief History of NLP\n",
    "\n",
    "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a fundamental goal of natural language processing.\n",
    "\n",
    "### Milestones in the History of NLP\n",
    "\n",
    "- **1950s**: The era of symbolic NLP, rule-based systems that tried to encode human knowledge and grammar rules into computers.\n",
    "- **1960s**: Development of the first chatbot, ELIZA, and further work on machine translation.\n",
    "- **1970s-1980s**: The rise of computational linguistics and the development of more sophisticated models for handling syntax and semantics.\n",
    "- **1990s**: Introduction of statistical NLP, leveraging large amounts of data and statistical methods to process language.\n",
    "- **2000s**: The emergence of machine learning in NLP, with systems beginning to learn from data rather than relying on hand-coded rules.\n",
    "- **2010s-Present**: The rise of deep learning has revolutionized NLP, leading to the development of models like BERT and GPT that can handle complex language tasks with unprecedented accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1313cd2-e658-4864-8a60-2f0dc263b96a",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is a critical step in NLP. It involves preparing and cleaning text data for further analysis and modeling. The goal is to simplify the text and remove any noise that might distract the machine learning algorithms from understanding the core content.\n",
    "\n",
    "Raw text data is often messy and unstructured, with various issues:\n",
    "\n",
    "- Irrelevant characters and symbols\n",
    "- Inconsistent formatting\n",
    "- Typos and spelling errors\n",
    "- Diverse languages and slang\n",
    "- Stopwords (commonly used words that may not be useful in analysis)\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, called *tokens*. Tokens can be words, numbers, or punctuation marks. It's the first step in turning unstructured text into a form that can be analyzed.\n",
    "\n",
    "### White-space Tokenization\n",
    "\n",
    "This is the simplest form of tokenization. It splits the text by white spaces, including spaces, tabs, and new line characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b71bf2-b565-4882-b092-5cca88a7b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Example usage:\n",
    "text = \"Natural language processing is fun.\"\n",
    "tokens = whitespace_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2701c3-c486-4eca-af0a-3de088d7f194",
   "metadata": {},
   "source": [
    "### Punctuation-based Tokenization\n",
    "\n",
    "This method not only splits by white spaces but also considers punctuation marks as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d89ec8-9f7c-4c8e-a0a1-933e194f5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def punctuation_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"Natural language processing is fun!\"\n",
    "tokens = punctuation_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dded96-252a-416d-8ea5-eee987770258",
   "metadata": {},
   "source": [
    "### Using NLP Libraries for Tokenization\n",
    "\n",
    "Libraries like `NLTK` and `spaCy` provide robust tokenization functions that handle edge cases and are more sophisticated than the simple white-space or punctuation-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899927f5-01e9-4a61-bec1-e9664930e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243c0d8-b212-4ab0-ab7f-5df1b1881403",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing is fun!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8dbed7-a987-49b2-97b3-3aec6b6f6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Download the spaCy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438537c7-65b9-4ba0-a9bb-70e81810f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57aa0f5-8945-4c4f-baac-d33aa11450a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing is fun!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21470992-244d-4579-8225-4cb6cf4afb9c",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "Text normalization involves transforming text into a more uniform format to improve the performance of text analysis algorithms. Two common text normalization techniques are *stemming* and *lemmatization*.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is a process of reducing words to their word stem, base, or root form—generally a written word form. The idea is to remove affixes (prefixes and suffixes) from words to get to the core meaning of the word.\n",
    "\n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This process is quite crude and a stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n",
    "\n",
    "### Porter Stemmer\n",
    "\n",
    "The Porter Stemming Algorithm is one of the oldest and most commonly used algorithms. It's designed for the English language and has a series of rules to determine the stripping of suffixes.\n",
    "\n",
    "### Snowball Stemmer\n",
    "\n",
    "The Snowball Stemmer, also known as the English Stemmer or Porter2 Stemmer, is a slightly improved version of the Porter stemmer and is part of a larger framework called Snowball. It offers stemmers for several languages besides English.\n",
    "\n",
    "### Advantages and Disadvantages of Stemming\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement and fast to run.\n",
    "- Reduces the corpus of words the model is exposed to.\n",
    "- Often improves the performance of text classification models.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Can produce stems that are not actual words.\n",
    "- Sometimes too aggressive, cutting off too much of the word and changing the meaning.\n",
    "- Does not consider the context of the word, which can lead to inaccuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b0031-0c3d-4008-aafa-8638b86028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ccb3c-13f2-4f56-8d90-cacdfd7359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d0860-992a-456b-87a4-a2af568c9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly', 'better', 'mice', 'feet']\n",
    "\n",
    "porter_stems = [porter.stem(word) for word in words]\n",
    "print(f\"Porter Stemmer: {porter_stems}\")\n",
    "\n",
    "snowball_stems = [snowball.stem(word) for word in words]\n",
    "print(f\"Snowball Stemmer: {snowball_stems}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3a6cd-45fe-4e87-9b58-9f2e3f28982b",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "While stemming often involves rule-based chopping of ends of words, lemmatization involves a linguistic approach to reduce a word to its base or root form. Lemmatization uses vocabulary and morphological analysis, often with the aid of part-of-speech tagging, to return the base or dictionary form of a word, known as the lemma.\n",
    "\n",
    "### The Role of Part-of-Speech Tagging in Lemmatization\n",
    "\n",
    "Part-of-speech (POS) tagging is crucial in lemmatization because many words have different lemmas based on their part of speech in a sentence. For example, the word \"saw\" can be a verb or a noun, and the lemma would differ accordingly (\"see\" for the verb, \"saw\" for the noun).\n",
    "\n",
    "`nltk` or `spacy` contain pre-trained models for POS tagging and lemmatization.\n",
    "\n",
    "### Advantages and Disadvantages of Lemmatization\n",
    "\n",
    "**Advantages:**\n",
    "- Produces lemmas, which are actual words, improving interpretability.\n",
    "- More accurate than stemming as it considers the context.\n",
    "\n",
    "**Disadvantages:**\n",
    "- More computationally expensive than stemming.\n",
    "- Requires additional information (POS tags).\n",
    "- May not improve performance significantly more than stemming for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee9e22-0907-4a5f-ba77-1ad2db3a87f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ee21f-7a5e-43fa-9f6c-7ce3d16c9883",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7136553-ced5-4280-bf20-7a5319c4b1f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to convert POS tag to a format recognized by the lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b374d0-0b19-4ae0-b426-beec511f22dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatize words with POS tags\n",
    "lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8b01d-d58d-4491-abf2-18bce5821101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use spacy on another example to demonstrate pos and lemmatization\n",
    "sentence = \"The striped bats are hanging on their feet for better sleep.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df23730-5b01-48e7-b343-18717682b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8159f-65cf-46e0-8abf-61da4d733d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging and lemmatization\n",
    "print(f\"{'Text':{8}} {'Lemma':{8}} {'POS':{6}} {'Tag':{6}} {'Explanation'}\")\n",
    "print()\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{8}} {token.lemma_:{8}} {token.pos_:{6}} {token.tag_:{6}} {spacy.explain(token.tag_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78d490-86b4-4faf-a120-fc757e1840bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the lemmatized form of each word\n",
    "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d3ca9-a395-46ab-a0dc-6ccf8115629c",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "Language modeling is a critical task that deals with predicting the probability of a sequence of words. It is used in various applications such as speech recognition, machine translation, and text generation.\n",
    "\n",
    "## Formula\n",
    "\n",
    "A language model is a probabilistic model that assigns a probability to a sequence of words, effectively capturing the likelihood that the sequence will occur in a language. In mathematical terms, given a sequence of words $ w_1, w_2, \\ldots, w_n $, the language model estimates the probability:\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) $$\n",
    "\n",
    "This probability can be decomposed using the chain rule of probability as:\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot \\ldots \\cdot P(w_n | w_1, w_2, \\ldots, w_{n-1}) $$\n",
    "\n",
    "## N-gram Models\n",
    "\n",
    "An n-gram is a contiguous sequence of $ n $ items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. In the context of language modeling, we are typically talking about words. It approximates the probability of a word sequence by only considering the $ n-1 $ previous words. This is known as the Markov assumption.\n",
    "\n",
    "### Unigram Models\n",
    "\n",
    "A unigram model is the simplest form of a statistical language model. It assumes that the probability of a word is independent of the words before it.\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) = P(w_1) \\cdot P(w_2) \\cdot \\ldots \\cdot P(w_n) $$\n",
    "\n",
    "### Bigram Models\n",
    "\n",
    "A bigram model, also known as a 2-gram model, assumes that the probability of a word depends only on the immediately preceding word.\n",
    "\n",
    "$$ P(w_n | w_1, w_2, \\ldots, w_{n-1}) \\approx P(w_n | w_{n-1}) = \\frac{Count(w_{n-1}, w_n)}{Count(w_{n-1})} $$\n",
    "\n",
    "### Trigram Models and Higher-Order Models\n",
    "\n",
    "Trigram models extend this to consider the two preceding words, and higher-order models consider more history. However, as the history increases, these models become more complex and require more data to estimate the probabilities accurately.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "**Sparsity**: As $ n $ increases, the likelihood of encountering unseen n-grams (those not present in the training corpus) increases, leading to sparsity.\n",
    "\n",
    "**Curse of Dimensionality**: The number of possible n-grams increases exponentially with $ n $, which leads to a combinatorial explosion in the number of parameters to be estimated.\n",
    "\n",
    "### Smoothing Techniques\n",
    "\n",
    "Smoothing techniques are used to handle the issue of zero probabilities for unseen n-grams. Common techniques include:\n",
    "\n",
    "- **Add-One (Laplace) Smoothing**: Adding one to all the n-gram counts.\n",
    "- **Add-k Smoothing**: Adding a small constant $ k $ to the counts.\n",
    "- **Backoff and Interpolation**: Using lower-order n-gram probabilities when higher-order n-grams have zero counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b59a7-418d-480b-9e3b-3c14f6e0eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e84026-51e3-438b-9426-7e3cc8ca44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8cae6-59b4-40a6-b9a2-7d0113d4f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokens = nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24f20d-06f0-43e3-b307-7db3e98c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bigram frequencies\n",
    "bigram_freqs = Counter(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f307c-accf-47f5-80e5-b77728b1cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of bigrams\n",
    "total_bigrams = sum(bigram_freqs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b4af3-5f8e-4766-b4d6-e156c5635b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bigram probabilities\n",
    "bigram_probs = {bigram: freq / total_bigrams for bigram, freq in bigram_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39622f-e3ad-45ac-94cd-fd82d70e3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display bigram probabilities\n",
    "for bigram, prob in bigram_probs.items():\n",
    "    print(f\"Probability of {bigram}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be94fb-c2f3-4219-a92a-85aa65cc5c0c",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "\n",
    "The Vector Space Model (VSM) is a mathematical model used to represent text documents as vectors of identifiers, such as index terms. It is used in information retrieval and text mining to measure the similarity between documents. In VSM, each dimension corresponds to a separate term, and the value in each dimension represents the significance of the term in the document.\n",
    "\n",
    "## Term-Document Matrix\n",
    "\n",
    "In VSM, a Term-Document Matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the collection of documents. In a Term-Document Matrix, rows correspond to terms in the corpus while columns correspond to documents. Each entry in this matrix denotes the frequency or the weight of a term in a document.\n",
    "\n",
    "Here's a simple example of a Term-Document Matrix for five documents:\n",
    "\n",
    "![tf](../assets/tf.png)\n",
    "\n",
    "Such matrices are often sparse since not all words appear in all documents.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "The TF-IDF value is calculated as follows:\n",
    "\n",
    "- **Term Frequency (TF)**, which measures how frequently a term occurs in a document. It is calculated as the number of times a term `t` appears in a document `d`, divided by the total number of terms in the document.\n",
    "\n",
    "$$ TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**, which measures how important a term is within the entire corpus. It is calculated as the logarithm of the number of documents in the corpus divided by the number of documents where the term `t` appears.\n",
    "\n",
    "$$ IDF(t, D) = \\log \\left( \\frac{\\text{Total number of documents in corpus } D}{\\text{Number of documents with term } t} \\right) $$\n",
    "\n",
    "- **TF-IDF**, the product of TF and IDF:\n",
    "\n",
    "$$ TF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D) $$\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is a measure used to determine how similar two documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. This metric is, therefore, a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The cosine similarity of two documents will range from 0 to 1, where 0 means no similarity and 1 means the same content.\n",
    "\n",
    "The formula for calculating the cosine similarity between two vectors $ A $ and $ B $ is:\n",
    "\n",
    "$$ \\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i \\times B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}} $$\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Assumes term independence, which is not always the case.\n",
    "- Does not capture the semantic relationship between words.\n",
    "- High-dimensional and sparse vectors due to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8953a-2faa-4755-b6dc-37f80fddd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6856ef-bedf-4c51-8f9b-db4f6a0d9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    'The sky is blue',\n",
    "    'The sun is bright',\n",
    "    'The sun in the sky is bright',\n",
    "    'We can see the shining sun, the bright sun'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0c98d-f291-4d27-a151-eebfcf8b05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5897a-7f40-4fb4-aae1-e01ce9397d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260969c-be3e-46b1-bb43-c897fa88bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866e768-a9be-4448-898e-26cf8487a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity between the 2nd document with all others\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1], tfidf_matrix)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92b21c-ecf0-469e-a761-1242ca42d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize another TF-IDF Vectorizer, with stop words removal\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c27285-3f68-443c-9062-316d5498a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31960d6a-7e0b-4790-a657-10dd24f9c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6f0c0-5c97-4a64-a69a-8a72573a0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity between the 2nd document with all others\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1], tfidf_matrix)\n",
    "print(cosine_similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
